"""
Dataset class for the Bowen et al. mouse auditory cortex calcium imaging dataset.

This script downloads and processes the original dataset introduced in:

[Citation] Bowen et al. (2024), "Fractured columnar small-world functional network
organization in volumes of L2/3 of mouse auditory cortex," PNAS Nexus, 3(2): pgae074.
https://doi.org/10.1093/pnasnexus/pgae074

We apply the preprocessing and graph-construction steps defined in this module to obtain
a representation of neuronal activity suitable for our experiments.

Please cite the original paper when using this dataset or any derivatives.
"""

import os
import os.path as osp
import shutil
from typing import ClassVar

import numpy as np
import pandas as pd
import scipy.io
import torch
from omegaconf import DictConfig
from scipy.sparse import linalg as sp_linalg
from torch_geometric.data import Data, InMemoryDataset, extract_zip
from torch_geometric.io import fs
from torch_geometric.utils import to_undirected

from topobench.data.utils import download_file_from_link
from topobench.data.utils.io_utils import collect_mat_files, process_mat


class A123CortexMDataset(InMemoryDataset):
    """A1 and A2/3 mouse auditory cortex dataset.

    Loads neural correlation data from mouse auditory cortex regions.

    Parameters
    ----------
    root : str
        Root directory where the dataset will be saved.
    name : str
        Name of the dataset.
    parameters : DictConfig
        Configuration parameters for the dataset including corr_threshold,
        n_bins, min_neurons, and hodge_k.

    Attributes
    ----------
    URLS : dict
        Dictionary containing the URLs for downloading the dataset.
    FILE_FORMAT : dict
        Dictionary containing the file formats for the dataset.
    RAW_FILE_NAMES : dict
        Dictionary containing the raw file names for the dataset.
    """

    URLS: ClassVar = {
        "Auditory cortex data": "https://gcell.umd.edu/data/Auditory_cortex_data.zip",
    }

    FILE_FORMAT: ClassVar = {
        "Auditory cortex data": "zip",
    }

    RAW_FILE_NAMES: ClassVar = {}

    def __init__(
        self,
        root: str,
        name: str,
        parameters: DictConfig,
    ) -> None:
        self.name = name
        self.parameters = parameters
        # defensive parameter access with sensible defaults
        try:
            self.corr_threshold = float(parameters.get("corr_threshold", 0.2))
        except Exception:
            self.corr_threshold = float(
                getattr(parameters, "corr_threshold", 0.2)
            )

        try:
            self.n_bins = int(parameters.get("n_bins", 9))
        except Exception:
            self.n_bins = int(getattr(parameters, "n_bins", 9))

        try:
            self.min_neurons = int(parameters.get("min_neurons", 8))
        except Exception:
            self.min_neurons = int(getattr(parameters, "min_neurons", 8))

        # optional parameter controlling how many eigenvalues to compute for Hodge L1
        try:
            self.hodge_k = int(parameters.get("hodge_k", 6))
        except Exception:
            self.hodge_k = int(getattr(parameters, "hodge_k", 6))

        self.session_map = {}
        super().__init__(
            root,
        )

        out = fs.torch_load(self.processed_paths[0])
        assert len(out) == 3 or len(out) == 4
        if len(out) == 3:  # Backward compatibility.
            data, self.slices, self.sizes = out
            data_cls = Data
        else:
            data, self.slices, self.sizes, data_cls = out

        if not isinstance(data, dict):  # Backward compatibility.
            self.data = data
        else:
            self.data = data_cls.from_dict(data)

        # For this dataset we don't assume the internal _data is a torch_geometric Data
        # (this dataset exposes helper methods to construct subgraphs on demand).

    def __repr__(self) -> str:
        return f"{self.name}(self.root={self.root}, self.name={self.name}, self.parameters={self.parameters}, self.force_reload={self.force_reload})"

    @property
    def raw_dir(self) -> str:
        """Path to the raw directory of the dataset.

        Returns
        -------
        str
            Path to the raw directory.
        """
        return osp.join(self.root, self.name, "raw")

    @property
    def processed_dir(self) -> str:
        """Path to the processed directory of the dataset.

        Returns
        -------
        str
            Path to the processed directory.
        """
        return osp.join(self.root, self.name, "processed")

    @property
    def raw_file_names(self) -> list[str]:
        """Return the raw file names for the dataset.

        Returns
        -------
        list[str]
            List of raw file names.
        """
        return ["Auditory cortex data/"]

    @property
    def processed_file_names(self) -> str:
        """Return the processed file name for the dataset.

        Returns
        -------
        str
            Processed file name.
        """
        return "data.pt"

    def download(self) -> None:
        """Download the dataset from a URL and extract to the raw directory."""
        # Download data from the source
        dataset_key = "Auditory cortex data"
        self.url = self.URLS[dataset_key]
        self.file_format = self.FILE_FORMAT[dataset_key]

        # Use self.name as the downloadable dataset name
        download_file_from_link(
            file_link=self.url,
            path_to_save=self.raw_dir,
            dataset_name=self.name,
            file_format=self.file_format,
            verify=False,
            timeout=60,  # 60 seconds per chunk read timeout
            retries=3,  # Retry up to 3 times
        )

        # Extract zip file
        folder = self.raw_dir
        filename = f"{self.name}.{self.file_format}"
        path = osp.join(folder, filename)
        extract_zip(path, folder)
        # Delete zip file
        os.unlink(path)

        # Move files from extracted "Auditory cortex data/" directory to raw_dir
        downloaded_dir = osp.join(folder, self.name)
        if osp.exists(downloaded_dir):
            for file in os.listdir(downloaded_dir):
                src = osp.join(downloaded_dir, file)
                dst = osp.join(folder, file)
                if osp.isdir(src):
                    shutil.copytree(src, dst, dirs_exist_ok=True)
                else:
                    shutil.move(src, dst)
            # Delete the extracted top-level directory
            shutil.rmtree(downloaded_dir)
        self.data_dir = folder

    @staticmethod
    def extract_samples(data_dir: str, n_bins: int, min_neurons: int = 8):
        """Extract subgraph samples from raw .mat files.

        Parameters
        ----------
        data_dir : str
            Directory containing the raw .mat files.
        n_bins : int
            Number of frequency bins to use for binning.
        min_neurons : int, optional
            Minimum number of neurons required per sample. Defaults to 8.

        Returns
        -------
        pd.DataFrame
            DataFrame containing extracted samples with columns for
            session_file, session_id, layer, bf_bin, neuron_indices,
            corr, and noise_corr.
        """
        mat_files = collect_mat_files(data_dir)

        samples = []
        session_id = 0
        for f in mat_files:
            print(f"Processing session {session_id}: {os.path.basename(f)}")
            mt = process_mat(scipy.io.loadmat(f))
            for layer in range(1, 6):
                scorrs = np.array(mt["selectZCorrInfo"]["SigCorrs"])
                ncorrs = np.array(mt["selectZCorrInfo"]["NoiseCorrsTrial"])
                bfvals = np.array(mt["BFInfo"][layer]["BFval"]).ravel()
                if scorrs.size == 0 or bfvals.size == 0:
                    continue

                bin_ids = bfvals.astype(int)

                for bin_idx in range(n_bins):
                    sel = np.where(bin_ids == bin_idx)[0]
                    if len(sel) < min_neurons:
                        continue
                    subcorr = scorrs[np.ix_(sel, sel)]
                    samples.append(
                        {
                            "session_file": f,
                            "session_id": session_id,
                            "layer": layer,
                            "bf_bin": int(bin_idx),
                            "neuron_indices": sel.tolist(),
                            "corr": subcorr.astype(float),
                            "noise_corr": ncorrs[np.ix_(sel, sel)].astype(
                                float
                            ),
                        }
                    )
            session_id += 1

        samples = pd.DataFrame(samples)
        return samples

    def _sample_to_pyg_data(
        self, sample: dict, threshold: float = 0.2
    ) -> Data:
        """Convert a sample dictionary to a PyTorch Geometric Data object.

        Converts correlation matrices to graph representation with node features
        and edges for graph-level classification tasks.

        Parameters
        ----------
        sample : dict
            Sample dictionary containing 'corr', 'noise_corr', 'session_id',
            'layer', and 'bf_bin' keys.
        threshold : float, optional
            Correlation threshold for creating edges. Defaults to 0.2.

        Returns
        -------
        torch_geometric.data.Data
            Data object with node features [mean_corr, std_corr, noise_diag],
            edges from thresholded correlation, and label y as integer bf_bin.
        """
        corr = np.asarray(sample.get("corr"))
        if corr.ndim != 2 or corr.size == 0:
            # empty placeholder graph
            x = torch.zeros((0, 3), dtype=torch.float)
            edge_index = torch.empty((2, 0), dtype=torch.long)
            edge_attr = torch.empty((0, 1), dtype=torch.float)
        else:
            n = corr.shape[0]
            # sanitize
            corr = np.nan_to_num(corr)

            mean_corr = corr.mean(axis=1)
            std_corr = corr.std(axis=1)
            noise_diag = np.zeros(n)
            if "noise_corr" in sample and sample["noise_corr"] is not None:
                nc = np.asarray(sample["noise_corr"])
                if nc.shape == corr.shape:
                    noise_diag = np.diag(nc)

            x_np = np.vstack([mean_corr, std_corr, noise_diag]).T
            x = torch.tensor(x_np, dtype=torch.float)

            # build edges from thresholded correlation (upper triangle)
            adj = (corr >= threshold).astype(int)
            iu = np.triu_indices(n, k=1)
            sel = np.where(adj[iu] == 1)[0]
            if sel.size == 0:
                edge_index = torch.empty((2, 0), dtype=torch.long)
                edge_attr = torch.empty((0, 1), dtype=torch.float)
            else:
                rows = iu[0][sel]
                cols = iu[1][sel]
                edge_index_np = np.vstack([rows, cols])
                edge_index = torch.tensor(edge_index_np, dtype=torch.long)
                # make undirected
                edge_index = to_undirected(edge_index)
                # edge_attr: corresponding corr weights (for both directions, if made undirected)
                weights = corr[rows, cols]
                weights = (
                    np.repeat(weights, 2)
                    if edge_index.size(1) == weights.size * 2
                    else weights
                )
                edge_attr = torch.tensor(
                    weights.reshape(-1, 1), dtype=torch.float
                )

        y = torch.tensor([int(sample.get("bf_bin", -1))], dtype=torch.long)
        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)
        # attach metadata
        data.session_id = int(sample.get("session_id", -1))
        data.layer = int(sample.get("layer", -1))

        # Compute Hodge L1 eigenvalues if hodge_k > 0
        hodge_l1 = self._compute_hodge_l1_eigenvalues(
            corr, threshold=threshold, k=self.hodge_k
        )
        if hodge_l1 is not None:
            data.hodge_l1 = hodge_l1

        return data

    def _compute_hodge_l1_eigenvalues(
        self, corr: np.ndarray, threshold: float = 0.2, k: int = 6
    ) -> torch.Tensor:
        """Compute k smallest Hodge L1 eigenvalues from correlation matrix.

        Hodge L1 is the Laplacian of the 1-skeleton (edge graph) of a clique
        complex constructed from the thresholded correlation matrix. This
        captures higher-order connectivity structure.

        Parameters
        ----------
        corr : np.ndarray
            Correlation matrix (n x n).
        threshold : float, optional
            Threshold for binarizing correlation to adjacency. Defaults to 0.2.
        k : int, optional
            Number of smallest eigenvalues to compute. Defaults to 6.

        Returns
        -------
        torch.Tensor
            Tensor of shape (min(k, n_edges),) containing k smallest eigenvalues,
            or None if the graph is too small or empty.
        """
        if k <= 0 or corr.size == 0:
            return None

        n = corr.shape[0]
        # Build adjacency from thresholded correlation
        adj = (corr >= threshold).astype(int)
        np.fill_diagonal(adj, 0)  # Remove self-loops

        # Count edges (triangles would be 1-skeleton of 2-cliques)
        # For simplicity, we compute the Laplacian of the edge graph directly
        # Edge graph: nodes are edges, two nodes connected if they share a vertex (form triangle)

        # Get edge list
        rows, cols = np.where(np.triu(adj, k=1))
        if len(rows) == 0:
            return None  # No edges

        num_edges = len(rows)
        # Compute incidence matrix (n_edges x n_nodes)
        incidence = np.zeros((num_edges, n), dtype=int)
        for i, (r, c) in enumerate(zip(rows, cols, strict=True)):
            incidence[i, r] = 1
            incidence[i, c] = 1

        # Hodge L1 = incidence.T @ incidence (upper adjacency of clique complex)
        # This gives the number of common neighbors (triangles) for each edge
        hodge_adj = incidence.T @ incidence - np.diag(
            np.diag(incidence.T @ incidence)
        )
        hodge_degree = hodge_adj.sum(axis=1)
        hodge_laplacian = np.diag(hodge_degree) - hodge_adj

        # Compute k smallest eigenvalues
        try:
            # Use sparse eigenvalue solver if available
            k_eigs = min(k, hodge_laplacian.shape[0] - 2)
            if k_eigs <= 0:
                return None

            eigenvalues, _ = sp_linalg.eigsh(
                hodge_laplacian, k=k_eigs, which="SM", maxiter=1000
            )
            eigenvalues = np.sort(eigenvalues)[:k_eigs]
            return torch.tensor(eigenvalues, dtype=torch.float32)
        except Exception:
            # Fallback if sparse solver fails
            try:
                eigenvalues = np.linalg.eigvalsh(hodge_laplacian)
                eigenvalues = np.sort(eigenvalues)[:k]
                return torch.tensor(eigenvalues, dtype=torch.float32)
            except Exception:
                return None

    def process(self) -> None:
        """Generate raw files into collated PyG dataset and save to disk.

        This implementation mirrors other datasets in the repo: it calls the
        static helper `extract_samples()` to enumerate subgraphs, converts each
        to a `torch_geometric.data.Data` object via `_sample_to_pyg_data()`,
        optionally computes/attaches topology vectors, collates and saves.
        """
        data_dir = self.raw_dir

        print(f"[A123] Processing dataset from: {data_dir}")
        print(f"[A123] Files in raw_dir: {os.listdir(data_dir)}")

        # extract sample descriptions
        print("[A123] Starting extract_samples()...")
        samples = A123CortexMDataset.extract_samples(
            data_dir, self.n_bins, self.min_neurons
        )

        print(f"[A123] Extracted {len(samples)} samples")

        data_list = []
        for idx, (_, s) in enumerate(samples.iterrows()):
            if idx % 100 == 0:
                print(
                    f"[A123] Converting sample {idx}/{len(samples)} to PyG Data..."
                )
            d = self._sample_to_pyg_data(s, threshold=self.corr_threshold)
            data_list.append(d)

        # collate and save processed dataset
        print(f"[A123] Collating {len(data_list)} samples...")
        self.data, self.slices = self.collate(data_list)
        self._data_list = None
        print(f"[A123] Saving processed data to {self.processed_paths[0]}...")
        fs.torch_save(
            (self._data.to_dict(), self.slices, {}, self._data.__class__),
            self.processed_paths[0],
        )
        print("[A123] Processing complete!")
