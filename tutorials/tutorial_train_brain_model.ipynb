{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af53c476",
   "metadata": {},
   "source": [
    "# Training TBModel on Auditory Cortex Data for 1 and 2/3 regions.\n",
    "\n",
    "This notebook demonstrates loading the MUTAG dataset, applying a simple lifting, defining a small backbone, and training a `TBModel` using `TBLoss` and `TBOptimizer`.\n",
    "\n",
    "Requirements: the project installed in PYTHONPATH and optional dependencies (torch_geometric, networkx, ripser/persim) if you want advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d0adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ed7f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports\n",
    "import torch\n",
    "import lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Data loading / preprocessing utilities from the repo\n",
    "from topobench.data.loaders.graph.a123_loader import A123DatasetLoader\n",
    "from topobench.dataloader.dataloader import TBDataloader\n",
    "from topobench.data.preprocessor import PreProcessor\n",
    "\n",
    "# Model / training building blocks\n",
    "from topobench.model.model import TBModel\n",
    "# example backbone building block (SCN2 is optional; we provide a tiny custom backbone below)\n",
    "# from topomodelx.nn.simplicial.scn2 import SCN2\n",
    "from topobench.nn.wrappers.simplicial import SCNWrapper\n",
    "from topobench.nn.encoders import AllCellFeatureEncoder\n",
    "from topobench.nn.readouts import PropagateSignalDown\n",
    "\n",
    "# Optimization / evaluation\n",
    "from topobench.loss.loss import TBLoss\n",
    "from topobench.optimizer import TBOptimizer\n",
    "from topobench.evaluator.evaluator import TBEvaluator\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03042d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs created\n"
     ]
    }
   ],
   "source": [
    "# 2) Configurations and utilities\n",
    "loader_config = {\n",
    "    'data_domain': 'graph',\n",
    "    'data_type': 'A123',\n",
    "    # the loader/dataset expects the dataset name key used in the dataset class\n",
    "    'data_name': 'a123_cortex_m',\n",
    "    'data_dir': './data/a123/'\n",
    "}\n",
    "\n",
    "# Transform config: single transform with transform_name and transform_type\n",
    "# PreProcessor expects either {\"transform_name\": ...} (single) or {\"key1\": {...}, \"key2\": {...}} (multiple)\n",
    "transform_config = {\n",
    "    'transform_type': 'lifting',\n",
    "    'transform_name': 'HypergraphKHopLifting',\n",
    "    'k_value': 1,\n",
    "}\n",
    "\n",
    "split_config = {\n",
    "    'learning_setting': 'inductive',\n",
    "    'split_type': 'random',\n",
    "    'data_seed': 0,\n",
    "    'data_split_dir': './data/a123/splits/',\n",
    "    'train_prop': 0.5,\n",
    "}\n",
    "\n",
    "# model / task hyperparameters\n",
    "# A123 sample node features are: [mean_corr, std_corr, noise_diag] => 3 channels\n",
    "in_channels = 3\n",
    "# Multiclass classification: 9 frequency bins (bf_bin 0-8)\n",
    "out_channels = 9\n",
    "dim_hidden = 16\n",
    "n_bins = 9  # default binning from extract_samples\n",
    "hodge_k = 10  # Number of Hodge L1 eigenvalues to use (from dataset config)\n",
    "\n",
    "readout_config = {\n",
    "    'readout_name': 'PropagateSignalDown',\n",
    "    'num_cell_dimensions': 1,\n",
    "    'hidden_dim': dim_hidden,\n",
    "    'out_channels': out_channels,\n",
    "    'task_level': 'graph',\n",
    "    'pooling_type': 'sum',\n",
    "}\n",
    "\n",
    "loss_config = {\n",
    "    'dataset_loss': {\n",
    "        'task': 'classification',\n",
    "        'loss_type': 'cross_entropy',\n",
    "    }\n",
    "}\n",
    "\n",
    "evaluator_config = {\n",
    "    'task': 'classification',\n",
    "    'num_classes': out_channels,\n",
    "    'metrics': ['accuracy', 'precision', 'recall'],\n",
    "}\n",
    "\n",
    "optimizer_config = {\n",
    "    'optimizer_id': 'Adam',\n",
    "    'parameters': {'lr': 0.001, 'weight_decay': 0.0005},\n",
    "}\n",
    "\n",
    "# convert to OmegaConf (the project often expects DictConfig)\n",
    "loader_config = OmegaConf.create(loader_config)\n",
    "transform_config = OmegaConf.create(transform_config)\n",
    "split_config = OmegaConf.create(split_config)\n",
    "readout_config = OmegaConf.create(readout_config)\n",
    "loss_config = OmegaConf.create(loss_config)\n",
    "evaluator_config = OmegaConf.create(evaluator_config)\n",
    "optimizer_config = OmegaConf.create(optimizer_config)\n",
    "\n",
    "print('Configs created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a33ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A123] Processing dataset from: data/a123_cortex_m/raw\n",
      "[A123] Files in raw_dir: ['Auditory cortex data', '__MACOSX']\n",
      "[A123] Starting extract_samples()...\n",
      "Processing session 0: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 1: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 1: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 2: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 2: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 3: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 3: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 4: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 4: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 5: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 5: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 6: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 6: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 7: allPlanesVariables27-Feb-2021.mat\n",
      "Processing session 7: allPlanesVariables27-Feb-2021.mat\n",
      "[A123] Extracted 250 samples\n",
      "[A123] Converting sample 0/250 to PyG Data...\n",
      "[A123] Extracted 250 samples\n",
      "[A123] Converting sample 0/250 to PyG Data...\n",
      "[A123] Converting sample 100/250 to PyG Data...\n",
      "[A123] Converting sample 100/250 to PyG Data...\n",
      "[A123] Converting sample 200/250 to PyG Data...\n",
      "[A123] Collating 250 samples...\n",
      "[A123] Saving processed data to data/a123_cortex_m/processed/data.pt...\n",
      "[A123] Processing complete!\n",
      "Dataset loaded\n",
      "[A123] Converting sample 200/250 to PyG Data...\n",
      "[A123] Collating 250 samples...\n",
      "[A123] Saving processed data to data/a123_cortex_m/processed/data.pt...\n",
      "[A123] Processing complete!\n",
      "Dataset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits created\n",
      "Datasets and datamodule ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 3) Loading the data\n",
    "\n",
    "# Use the A123-specific loader (A123DatasetLoader) to construct the dataset\n",
    "graph_loader = A123DatasetLoader(loader_config)\n",
    "\n",
    "dataset, dataset_dir = graph_loader.load()\n",
    "print('Dataset loaded')\n",
    "\n",
    "preprocessor = PreProcessor(dataset, dataset_dir, transform_config)\n",
    "dataset_train, dataset_val, dataset_test = preprocessor.load_dataset_splits(split_config)\n",
    "print('Dataset splits created')\n",
    "\n",
    "# create the TopoBench datamodule / dataloader wrappers\n",
    "datamodule = TBDataloader(dataset_train, dataset_val, dataset_test, batch_size=32)\n",
    "\n",
    "print('Datasets and datamodule ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7bc4a8",
   "metadata": {},
   "source": [
    "## 4) Backbone definition\n",
    "\n",
    "We implement a tiny backbone as a `pl.LightningModule` which computes node and hyperedge features: $X_1 = B_1 dot X_0$ and applies two linear layers with ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f936b12",
   "metadata": {},
   "source": [
    "### Using Hodge L1 Topological Features\n",
    "\n",
    "The A123 dataset computes **Hodge L1 eigenvalues** for each graph sample during processing (controlled by the `hodge_k` parameter in config). These topological features are attached to each `Data` object as `data.hodge_l1` and are automatically batched into `batch.hodge_l1` during training.\n",
    "\n",
    "To use Hodge L1 features in your model:\n",
    "1. Access `batch.hodge_l1` (shape: `[batch_size, hodge_k]`) in your backbone or readout forward pass\n",
    "2. Process it as additional graph-level information (e.g., concatenate with pooled node features or pass to a separate MLP)\n",
    "3. Fuse with the main graph embedding before the final classifier\n",
    "\n",
    "Example usage in a custom backbone or readout:\n",
    "```python\n",
    "if hasattr(batch, 'hodge_l1') and batch.hodge_l1 is not None:\n",
    "    # hodge_l1 shape: [batch_size, hodge_k]\n",
    "    hodge_features = batch.hodge_l1\n",
    "    # Process and fuse with other features as needed\n",
    "```\n",
    "\n",
    "See `configs/dataset/graph/a123.yaml` for the `hodge_k` parameter configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: ['__abstractmethods__', '__annotations__', '__call__', '__cat_dim__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__inc__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_edge_attr_cls', '_edge_to_layout', '_edges_to_layout', '_find_parent', '_get_edge_index', '_get_tensor', '_get_tensor_size', '_multi_get_tensor', '_put_edge_index', '_put_tensor', '_remove_edge_index', '_remove_tensor', '_store', '_tensor_attr_cls', '_to_type', '_union', 'apply', 'apply_', 'batch', 'batch_size', 'clone', 'coalesce', 'concat', 'connected_components', 'contains_isolated_nodes', 'contains_self_loops', 'contiguous', 'coo', 'cpu', 'csc', 'csr', 'cuda', 'debug', 'detach', 'detach_', 'edge_attr', 'edge_attrs', 'edge_index', 'edge_stores', 'edge_subgraph', 'edge_weight', 'face', 'from_data_list', 'from_dict', 'generate_ids', 'get_all_edge_attrs', 'get_all_tensor_attrs', 'get_edge_index', 'get_example', 'get_tensor', 'get_tensor_size', 'has_isolated_nodes', 'has_self_loops', 'index_select', 'is_coalesced', 'is_cuda', 'is_directed', 'is_edge_attr', 'is_node_attr', 'is_sorted', 'is_sorted_by_time', 'is_undirected', 'is_valid', 'keys', 'multi_get_tensor', 'node_attrs', 'node_offsets', 'node_stores', 'num_edge_features', 'num_edge_types', 'num_edges', 'num_faces', 'num_features', 'num_graphs', 'num_node_features', 'num_node_types', 'num_nodes', 'pin_memory', 'pos', 'put_edge_index', 'put_tensor', 'record_stream', 'remove_edge_index', 'remove_tensor', 'requires_grad_', 'share_memory_', 'size', 'snapshot', 'sort', 'sort_by_time', 'stores', 'stores_as', 'subgraph', 'time', 'to', 'to_data_list', 'to_dict', 'to_heterogeneous', 'to_namedtuple', 'up_to', 'update', 'update_tensor', 'validate', 'view', 'x', 'y']\n",
      "hodge_l1 shape: torch.Size([192])\n",
      "hodge_l1 sample:\n",
      "5.655198531684391e-16\n"
     ]
    }
   ],
   "source": [
    "# Quick check: Verify that hodge_l1 features are present in a batch\n",
    "# Uncomment to inspect batch contents during development\n",
    "\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "print(\"Batch keys:\", dir(batch))\n",
    "if hasattr(batch, 'hodge_l1'):\n",
    "    print(f\"hodge_l1 shape: {batch.hodge_l1.shape}\")  # Expected: [batch_size, hodge_k]\n",
    "    print(f\"hodge_l1 sample:\\n{batch.hodge_l1[0]}\")\n",
    "else:\n",
    "    print(\"hodge_l1 not found in batch (may need to reprocess dataset)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275c748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone defined\n"
     ]
    }
   ],
   "source": [
    "# class MyBackbone(pl.LightningModule):\n",
    "#     def __init__(self, dim_hidden):\n",
    "#         super().__init__()\n",
    "#         self.linear_0 = torch.nn.Linear(dim_hidden, dim_hidden)\n",
    "#         self.linear_1 = torch.nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "#     def forward(self, batch):\n",
    "#         # batch.x_0: node features (dense tensor of shape [N, dim_hidden])\n",
    "#         # batch.incidence_hyperedges: sparse incidence matrix with shape [m, n] or [n, m] depending on preprocessor convention\n",
    "#         x_0 = batch.x_0\n",
    "#         incidence_hyperedges = getattr(batch, 'incidence_hyperedges', None)\n",
    "#         if incidence_hyperedges is None:\n",
    "#             # fallback: try incidence as batch.incidence if available\n",
    "#             incidence_hyperedges = getattr(batch, 'incidence', None)\n",
    "\n",
    "#         # compute hyperedge features X_1 = B_1 dot X_0 (we assume B_1 is sparse and transposed appropriately)\n",
    "#         x_1 = None\n",
    "#         if incidence_hyperedges is not None:\n",
    "#             try:\n",
    "#                 x_1 = torch.sparse.mm(incidence_hyperedges, x_0)\n",
    "#             except Exception:\n",
    "#                 # if orientation differs, try transpose\n",
    "#                 x_1 = torch.sparse.mm(incidence_hyperedges.T, x_0)\n",
    "#         else:\n",
    "#             # no incidence available: create a zero hyperedge feature placeholder\n",
    "#             x_1 = torch.zeros_like(x_0)\n",
    "\n",
    "#         x_0 = self.linear_0(x_0)\n",
    "#         x_0 = torch.relu(x_0)\n",
    "\n",
    "#         x_1 = self.linear_1(x_1)\n",
    "#         x_1 = torch.relu(x_1)\n",
    "\n",
    "#         model_out = {'labels': batch.y, 'batch_0': getattr(batch, 'batch_0', None)}\n",
    "#         model_out['x_0'] = x_0\n",
    "#         model_out['hyperedge'] = x_1\n",
    "#         return model_out\n",
    "\n",
    "# print('Backbone defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3f0391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Backbone with Hodge Feature Fusion defined (strict validation mode)\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED: Properly fuse Hodge features with node embeddings\n",
    "class MyBackbone(pl.LightningModule):\n",
    "    \"\"\"Backbone that properly integrates Hodge L1 features into node embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim_hidden, hodge_k=10, use_hodge=True):\n",
    "        super().__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.config_hodge_k = hodge_k  # Store config value\n",
    "        self.use_hodge = use_hodge\n",
    "        self.actual_hodge_k = None  # Will be validated on first forward pass\n",
    "        self.layers_initialized = False\n",
    "        \n",
    "        # Create dummy linear layers - will be recreated with correct dimensions on first forward pass\n",
    "        self.linear_0 = torch.nn.Linear(dim_hidden, dim_hidden)\n",
    "        self.linear_1 = torch.nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def _initialize_layers(self, x_0_shape, hodge_k):\n",
    "        \"\"\"Initialize linear layers with correct dimensions based on data.\"\"\"\n",
    "        if self.layers_initialized:\n",
    "            return\n",
    "        \n",
    "        in_features_0 = x_0_shape[1] + (hodge_k if self.use_hodge else 0)\n",
    "        \n",
    "        # Recreate linear layers with correct input dimensions\n",
    "        self.linear_0 = torch.nn.Linear(in_features_0, self.dim_hidden)\n",
    "        self.linear_1 = torch.nn.Linear(self.dim_hidden, self.dim_hidden)\n",
    "        \n",
    "        # Move to same device as input\n",
    "        self.linear_0 = self.linear_0.to(x_0_shape[0].device if isinstance(x_0_shape[0], torch.Tensor) else 'cpu')\n",
    "        self.linear_1 = self.linear_1.to(x_0_shape[0].device if isinstance(x_0_shape[0], torch.Tensor) else 'cpu')\n",
    "        \n",
    "        self.layers_initialized = True\n",
    "        print(f\"[Backbone] Initialized layers: linear_0({in_features_0}→{self.dim_hidden}), linear_1({self.dim_hidden}→{self.dim_hidden})\")\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x_0 = batch.x_0  # Shape: [num_nodes, dim_hidden]\n",
    "        batch_0 = getattr(batch, 'batch_0', None)\n",
    "        \n",
    "        # Store original x_0 for hyperedge computation\n",
    "        x_0_original = x_0\n",
    "        x_0_augmented = x_0\n",
    "        \n",
    "        # Augment with Hodge features if enabled\n",
    "        if self.use_hodge and hasattr(batch, 'hodge_l1') and batch.hodge_l1 is not None:\n",
    "            hodge_l1 = batch.hodge_l1\n",
    "            batch_size = batch.num_graphs if hasattr(batch, 'num_graphs') else batch.y.shape[0]\n",
    "            \n",
    "            # Validate hodge_k on first pass (strict mode - no inference)\n",
    "            if self.actual_hodge_k is None:\n",
    "                if hodge_l1.dim() == 1:\n",
    "                    inferred_hodge_k = hodge_l1.shape[0] // batch_size\n",
    "                else:\n",
    "                    inferred_hodge_k = hodge_l1.shape[1]\n",
    "                \n",
    "                # Check if it matches config - raise error if mismatch\n",
    "                if inferred_hodge_k != self.config_hodge_k:\n",
    "                    raise ValueError(\n",
    "                        f\"\\n{'='*70}\\n\"\n",
    "                        f\"HODGE FEATURES CONFIGURATION MISMATCH\\n\"\n",
    "                        f\"{'='*70}\\n\"\n",
    "                        f\"Config hodge_k:  {self.config_hodge_k}\\n\"\n",
    "                        f\"Actual hodge_k:  {inferred_hodge_k}\\n\"\n",
    "                        f\"{'='*70}\\n\"\n",
    "                        f\"\\nPossible causes:\\n\"\n",
    "                        f\"  1. Dataset was preprocessed with hodge_k={inferred_hodge_k}\\n\"\n",
    "                        f\"  2. Config parameter doesn't match the data\\n\"\n",
    "                        f\"\\nSolutions:\\n\"\n",
    "                        f\"  A) Update config: hodge_k={inferred_hodge_k}\\n\"\n",
    "                        f\"  B) Disable Hodge features: use_hodge=False\\n\"\n",
    "                        f\"  C) Reprocess dataset with hodge_k={self.config_hodge_k}\\n\"\n",
    "                        f\"{'='*70}\\n\"\n",
    "                    )\n",
    "                \n",
    "                self.actual_hodge_k = inferred_hodge_k\n",
    "                # Initialize layers if this is first forward pass\n",
    "                if not self.layers_initialized:\n",
    "                    self._initialize_layers(x_0.shape, self.actual_hodge_k)\n",
    "                    print(f\"[Backbone] Hodge features validated: hodge_k={self.actual_hodge_k} ✓\")\n",
    "            \n",
    "            # Reshape and broadcast Hodge features\n",
    "            if hodge_l1.dim() == 1:\n",
    "                hodge_l1 = hodge_l1.reshape(batch_size, self.actual_hodge_k)\n",
    "            \n",
    "            hodge_expanded = hodge_l1[batch_0]  # [num_nodes, actual_hodge_k]\n",
    "            x_0_augmented = torch.cat([x_0, hodge_expanded], dim=1)\n",
    "        else:\n",
    "            # Initialize layers even without Hodge features\n",
    "            if not self.layers_initialized:\n",
    "                self._initialize_layers(x_0.shape, 0)\n",
    "        \n",
    "        # Get incidence matrix\n",
    "        incidence_hyperedges = getattr(batch, 'incidence_hyperedges', None)\n",
    "        if incidence_hyperedges is None:\n",
    "            incidence_hyperedges = getattr(batch, 'incidence', None)\n",
    "\n",
    "        # Compute hyperedge features from ORIGINAL x_0\n",
    "        if incidence_hyperedges is not None:\n",
    "            try:\n",
    "                x_1 = torch.sparse.mm(incidence_hyperedges, x_0_original)\n",
    "            except Exception:\n",
    "                x_1 = torch.sparse.mm(incidence_hyperedges.T, x_0_original)\n",
    "        else:\n",
    "            x_1 = torch.zeros(x_0_original.shape[0], self.dim_hidden, device=x_0_original.device)\n",
    "\n",
    "        # Apply transformations\n",
    "        x_0_out = self.linear_0(x_0_augmented)\n",
    "        x_0_out = torch.relu(x_0_out)\n",
    "\n",
    "        x_1 = self.linear_1(x_1)\n",
    "        x_1 = torch.relu(x_1)\n",
    "\n",
    "        model_out = {\n",
    "            'labels': batch.y,\n",
    "            'batch_0': batch_0,\n",
    "            'x_0': x_0_out,\n",
    "            'hyperedge': x_1,\n",
    "        }\n",
    "        return model_out\n",
    "\n",
    "print('Improved Backbone with Hodge Feature Fusion defined (strict validation mode)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components instantiated (Hodge features disabled for now)\n"
     ]
    }
   ],
   "source": [
    "# 5) Model initialization (components)\n",
    "# Enable Hodge features - backbone will validate hodge_k on first forward pass\n",
    "# If hodge_k mismatch detected, will raise ValueError with helpful suggestions\n",
    "backbone = MyBackbone(dim_hidden, hodge_k=hodge_k, use_hodge=True)\n",
    "readout = PropagateSignalDown(**readout_config)\n",
    "loss = TBLoss(**loss_config)\n",
    "feature_encoder = AllCellFeatureEncoder(in_channels=[in_channels], out_channels=dim_hidden)\n",
    "evaluator = TBEvaluator(**evaluator_config)\n",
    "optimizer = TBOptimizer(**optimizer_config)\n",
    "\n",
    "print('Components instantiated (Hodge features ENABLED with strict validation)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "366a4200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBModel(backbone=MyBackbone(\n",
      "  (linear_0): Linear(in_features=26, out_features=16, bias=True)\n",
      "  (linear_1): Linear(in_features=16, out_features=16, bias=True)\n",
      "), readout=PropagateSignalDown(num_cell_dimensions=0, self.hidden_dim=16, readout_name=PropagateSignalDown, loss=TBLoss(losses=[DatasetLoss(task=classification, loss_type=cross_entropy)]), feature_encoder=AllCellFeatureEncoder(in_channels=[3], out_channels=16, dimensions=range(0, 1)))\n"
     ]
    }
   ],
   "source": [
    "# 6) Instantiate TBModel\n",
    "model = TBModel(backbone=backbone,\n",
    "                backbone_wrapper=None,\n",
    "                readout=readout,\n",
    "                loss=loss,\n",
    "                feature_encoder=feature_encoder,\n",
    "                evaluator=evaluator,\n",
    "                optimizer=optimizer,\n",
    "                compile=False)\n",
    "\n",
    "# Print a short summary (repr) to verify construction\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a81da250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariayuffa/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/mariayuffa/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb7bac73d284245983c8ebcff054094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariayuffa/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/mariayuffa/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f1ded4fda04524afab1c4acc64f4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      4\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, category=\u001b[38;5;167;01mUserWarning\u001b[39;00m, module=\u001b[33m'\u001b[39m\u001b[33mtorchmetrics\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m trainer = pl.Trainer(\n\u001b[32m      7\u001b[39m     max_epochs=\u001b[32m50\u001b[39m,  \u001b[38;5;66;03m# reduced for faster iteration\u001b[39;00m\n\u001b[32m      8\u001b[39m     accelerator=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     enable_model_summary=\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# skip the model summary printout\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m train_metrics = trainer.callback_metrics\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining finished. Collected metrics:\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    568\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    570\u001b[39m     ckpt_path,\n\u001b[32m    571\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    572\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    573\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    577\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    978\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    986\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._restarting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    142\u001b[39m         \u001b[38;5;28mself\u001b[39m._restarting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    249\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    252\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m         closure()\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m result = closure.consume_result()\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    278\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:167\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    170\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1306\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1277\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1280\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1281\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1282\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1283\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1284\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1304\u001b[39m \n\u001b[32m   1305\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1306\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:153\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:238\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:122\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    386\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    387\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    388\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    389\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    394\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m'\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     75\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/torch/optim/adam.py:148\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    151\u001b[39m     params_with_grad = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:108\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_closure\u001b[39m(\n\u001b[32m     96\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     97\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     98\u001b[39m     optimizer: Steppable,\n\u001b[32m     99\u001b[39m     closure: Callable[[], Any],\n\u001b[32m    100\u001b[39m ) -> Any:\n\u001b[32m    101\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[33;03m    hook is called.\u001b[39;00m\n\u001b[32m    103\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    106\u001b[39m \n\u001b[32m    107\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:144\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:129\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    132\u001b[39m         \u001b[38;5;28mself\u001b[39m.warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:317\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[32m    307\u001b[39m \n\u001b[32m    308\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m \n\u001b[32m    314\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    315\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer.world_size > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:319\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    322\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:390\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TopoBenchForNeuro/topobench/model/model.py:156\u001b[39m, in \u001b[36mTBModel.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Perform a single training step on a batch of data.\u001b[39;00m\n\u001b[32m    142\u001b[39m \n\u001b[32m    143\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m \u001b[33;03m    A tensor of losses between model predictions and targets.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m.state_str = \u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m model_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Update and log metrics\u001b[39;00m\n\u001b[32m    159\u001b[39m loss_value = model_out[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m].item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TopoBenchForNeuro/topobench/model/model.py:125\u001b[39m, in \u001b[36mTBModel.model_step\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    122\u001b[39m batch[\u001b[33m\"\u001b[39m\u001b[33mmodel_state\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.state_str\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m model_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[32m    128\u001b[39m model_out = \u001b[38;5;28mself\u001b[39m.process_outputs(model_out=model_out, batch=batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TopoBenchForNeuro/topobench/model/model.py:101\u001b[39m, in \u001b[36mTBModel.forward\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m     98\u001b[39m model_out = \u001b[38;5;28mself\u001b[39m.feature_encoder(batch)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Domain model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m model_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Readout\u001b[39;00m\n\u001b[32m    104\u001b[39m model_out = \u001b[38;5;28mself\u001b[39m.readout(model_out=model_out, batch=batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mMyBackbone.forward\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m     70\u001b[39m     x_1 = torch.zeros(x_0_original.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.linear_1.in_features, device=x_0_original.device)\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Apply linear transformations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m x_0 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m x_0 = torch.relu(x_0)\n\u001b[32m     76\u001b[39m x_1 = \u001b[38;5;28mself\u001b[39m.linear_1(x_1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tb/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd."
     ]
    }
   ],
   "source": [
    "# 7) Training loop (Lightning trainer)\n",
    "# Suppress some warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torchmetrics')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # reduced for faster iteration\n",
    "    accelerator='cpu',\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1,\n",
    "    enable_model_summary=False,  # skip the model summary printout\n",
    ")\n",
    "trainer.fit(model, datamodule)\n",
    "train_metrics = trainer.callback_metrics\n",
    "\n",
    "print('\\nTraining finished. Collected metrics:')\n",
    "for key, val in train_metrics.items():\n",
    "    try:\n",
    "        print(f'{key:25s} {float(val):.4f}')\n",
    "    except Exception:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca88d7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariayuffa/anaconda3/envs/tb/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b27a3c7441b42d4b54d330386635192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Backbone] Inferred hodge_k=6 (config had 10)\n",
      "[Backbone] Recreated linear_0 with in_features=22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.0476190485060215     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     36.33069610595703     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.01075268816202879    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test/recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.06666667014360428    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.0476190485060215    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    36.33069610595703    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.01075268816202879   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test/recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.06666667014360428   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test metrics:\n",
      "test/loss                 36.3307\n",
      "test/accuracy             0.0476\n",
      "test/precision            0.0108\n",
      "test/recall               0.0667\n"
     ]
    }
   ],
   "source": [
    "# 8) Testing and printing metrics\n",
    "trainer.test(model, datamodule)\n",
    "test_metrics = trainer.callback_metrics\n",
    "print('\\nTest metrics:')\n",
    "for key, val in test_metrics.items():\n",
    "    try:\n",
    "        print(f'{key:25s} {float(val):.4f}')\n",
    "    except Exception:\n",
    "        print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff71ffb",
   "metadata": {},
   "source": [
    "### Without Hodge features (100 epochs)\n",
    "\n",
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
    "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
    "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2222222238779068     </span>│\n",
    "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.0435843467712402     </span>│\n",
    "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.05868902429938316    </span>│\n",
    "│<span style=\"color: #008080; text-decoration-color: #008080\">        test/recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16466346383094788    </span>│\n",
    "└───────────────────────────┴───────────────────────────┘\n",
    "</pre>\n",
    "\n",
    "\n",
    "Test metrics:\n",
    "\n",
    "test/loss                 2.0436\n",
    "\n",
    "test/accuracy             0.2222\n",
    "\n",
    "test/precision            0.0587\n",
    "\n",
    "test/recall               0.1647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d2a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f71e10b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEBUGGING BACKBONE SHAPES\n",
      "============================================================\n",
      "\n",
      "Batch info:\n",
      "  batch.x_0 shape: torch.Size([778, 3])\n",
      "  batch.y shape: torch.Size([32])\n",
      "  batch.batch_0 shape: torch.Size([778])\n",
      "  num_graphs: 32\n",
      "  batch.hodge_l1 shape: torch.Size([192])\n",
      "  batch.hodge_l1 type: <class 'torch.Tensor'>\n",
      "  batch.incidence_hyperedges shape: torch.Size([778, 778])\n",
      "\n",
      "Backbone __init__ parameters:\n",
      "  dim_hidden: 16\n",
      "  hodge_k: 10\n",
      "  use_hodge: False\n",
      "  backbone.linear_0.in_features: 16\n",
      "  backbone.linear_0.out_features: 16\n",
      "  backbone.linear_1.in_features: 16\n",
      "  backbone.linear_1.out_features: 16\n",
      "\n",
      "--- Feature Encoder Stage ---\n",
      "After feature encoder:\n",
      "  model_out_encoded['x_0'] shape: torch.Size([778, 16])\n",
      "  model_out_encoded keys: ['test_mask', 'edge_index', 'session_id', 'val_mask', 'batch_0', 'ptr', 'y', 'batch_hyperedges', 'train_mask', 'num_hyperedges', 'edge_attr', 'layer', 'hodge_l1', 'x_0', 'x', 'x_hyperedges', 'incidence_hyperedges']\n",
      "\n",
      "--- Backbone Stage ---\n",
      "✓ Backbone forward pass successful!\n",
      "  model_out['x_0'] shape: torch.Size([778, 16])\n",
      "  model_out['hyperedge'] shape: torch.Size([778, 16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DEBUG: Check backbone shapes before training\n",
    "print(\"=\"*60)\n",
    "print(\"DEBUGGING BACKBONE SHAPES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch = next(iter(datamodule.val_dataloader()))\n",
    "print(f\"\\nBatch info:\")\n",
    "print(f\"  batch.x_0 shape: {batch.x_0.shape}\")\n",
    "print(f\"  batch.y shape: {batch.y.shape}\")\n",
    "print(f\"  batch.batch_0 shape: {batch.batch_0.shape}\")\n",
    "print(f\"  num_graphs: {batch.num_graphs}\")\n",
    "if hasattr(batch, 'hodge_l1'):\n",
    "    print(f\"  batch.hodge_l1 shape: {batch.hodge_l1.shape}\")\n",
    "    print(f\"  batch.hodge_l1 type: {type(batch.hodge_l1)}\")\n",
    "if hasattr(batch, 'incidence_hyperedges'):\n",
    "    print(f\"  batch.incidence_hyperedges shape: {batch.incidence_hyperedges.shape}\")\n",
    "elif hasattr(batch, 'incidence'):\n",
    "    print(f\"  batch.incidence shape: {batch.incidence.shape}\")\n",
    "\n",
    "print(f\"\\nBackbone __init__ parameters:\")\n",
    "print(f\"  dim_hidden: {dim_hidden}\")\n",
    "print(f\"  hodge_k: {hodge_k}\")\n",
    "print(f\"  use_hodge: {backbone.use_hodge}\")\n",
    "print(f\"  backbone.linear_0.in_features: {backbone.linear_0.in_features}\")\n",
    "print(f\"  backbone.linear_0.out_features: {backbone.linear_0.out_features}\")\n",
    "print(f\"  backbone.linear_1.in_features: {backbone.linear_1.in_features}\")\n",
    "print(f\"  backbone.linear_1.out_features: {backbone.linear_1.out_features}\")\n",
    "\n",
    "# First, run feature encoder\n",
    "print(f\"\\n--- Feature Encoder Stage ---\")\n",
    "model_out_encoded = feature_encoder(batch)\n",
    "print(f\"After feature encoder:\")\n",
    "print(f\"  model_out_encoded['x_0'] shape: {model_out_encoded.get('x_0', batch.x_0).shape}\")\n",
    "print(f\"  model_out_encoded keys: {list(model_out_encoded.keys())}\")\n",
    "\n",
    "# Test backbone alone on encoded features\n",
    "print(f\"\\n--- Backbone Stage ---\")\n",
    "try:\n",
    "    model_out_backbone = backbone(model_out_encoded)\n",
    "    print(f\"✓ Backbone forward pass successful!\")\n",
    "    print(f\"  model_out['x_0'] shape: {model_out_backbone['x_0'].shape}\")\n",
    "    print(f\"  model_out['hyperedge'] shape: {model_out_backbone['hyperedge'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Backbone forward pass failed:\")\n",
    "    print(f\"  Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82d8f2",
   "metadata": {},
   "source": [
    "## Why Hodge L1 Features Lead to Overfitting\n",
    "\n",
    "### Problems with the Current Implementation:\n",
    "\n",
    "1. **Disconnected Features**: The `hodge_emb` is computed but never used in the readout or loss\n",
    "   - It adds learnable parameters that aren't tied to the prediction task\n",
    "   - Model can overfit to training noise without helping generalization\n",
    "\n",
    "2. **Improper Integration**: Features aren't fused with the main prediction pathway\n",
    "   - The readout only uses `x_0`, `x_1`, etc. cell features\n",
    "   - Hodge features are isolated and ignored during inference\n",
    "\n",
    "3. **Batching Issues**: Hodge features concatenate incorrectly\n",
    "   - Training batches may cause feature-label misalignment\n",
    "   - Individual graph-level features get flattened into a 1D tensor\n",
    "\n",
    "### Solutions:\n",
    "\n",
    "#### Option 1: Properly Fuse Hodge Features into Node Embeddings (Recommended)\n",
    "Instead of computing separate `hodge_emb`, **augment the initial node features** with Hodge information:\n",
    "\n",
    "```python\n",
    "class MyBackbone(pl.LightningModule):\n",
    "    def __init__(self, dim_hidden, hodge_k=10):\n",
    "        super().__init__()\n",
    "        self.linear_0 = torch.nn.Linear(dim_hidden + hodge_k, dim_hidden)  # Include hodge_k\n",
    "        self.linear_1 = torch.nn.Linear(dim_hidden, dim_hidden)\n",
    "        self.hodge_k = hodge_k\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x_0 = batch.x_0\n",
    "        batch_size = batch.num_graphs if hasattr(batch, 'num_graphs') else batch.y.shape[0]\n",
    "        \n",
    "        # Expand hodge features to match node dimension\n",
    "        hodge_l1 = None\n",
    "        if hasattr(batch, 'hodge_l1') and batch.hodge_l1 is not None:\n",
    "            # Reshape from flattened to [batch_size, hodge_k]\n",
    "            hodge_l1 = batch.hodge_l1.reshape(batch_size, -1)\n",
    "            \n",
    "            # Broadcast to all nodes in each graph\n",
    "            batch_0 = batch.batch_0\n",
    "            hodge_expanded = hodge_l1[batch_0]  # [num_nodes, hodge_k]\n",
    "            \n",
    "            # Concatenate with node features\n",
    "            x_0 = torch.cat([x_0, hodge_expanded], dim=1)\n",
    "        \n",
    "        # Now x_0 has shape [num_nodes, dim_hidden + hodge_k]\n",
    "        x_0 = self.linear_0(x_0)\n",
    "        x_0 = torch.relu(x_0)\n",
    "        # ... rest of backbone\n",
    "```\n",
    "\n",
    "#### Option 2: Use Hodge Features as Graph-Level Auxiliary Information\n",
    "Fuse Hodge features after graph-level pooling in the readout or create a custom readout.\n",
    "\n",
    "#### Option 3: Skip Hodge Features for Now\n",
    "If integration is complex, simply don't use them. The base model performs reasonably without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8166ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPROVED: Properly fuse Hodge features with node embeddings\n",
    "class MyBackboneWithHodgeFusion(pl.LightningModule):\n",
    "    \"\"\"Backbone that properly integrates Hodge L1 features into node embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim_hidden, hodge_k=10, use_hodge=True):\n",
    "        super().__init__()\n",
    "        self.hodge_k = hodge_k\n",
    "        self.use_hodge = use_hodge\n",
    "        \n",
    "        # Input dimension depends on whether we use hodge features\n",
    "        in_dim = dim_hidden + (hodge_k if use_hodge else 0)\n",
    "        \n",
    "        self.linear_0 = torch.nn.Linear(in_dim, dim_hidden)\n",
    "        self.linear_1 = torch.nn.Linear(dim_hidden, dim_hidden)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x_0 = batch.x_0  # Shape: [num_nodes, dim_hidden]\n",
    "        \n",
    "        # Optionally augment node features with graph-level Hodge features\n",
    "        if self.use_hodge and hasattr(batch, 'hodge_l1') and batch.hodge_l1 is not None:\n",
    "            # Get batch structure\n",
    "            batch_0 = batch.batch_0  # Maps nodes to graphs\n",
    "            batch_size = batch.num_graphs if hasattr(batch, 'num_graphs') else batch.y.shape[0]\n",
    "            \n",
    "            # Reshape hodge_l1 from [batch_size * hodge_k] to [batch_size, hodge_k]\n",
    "            hodge_l1 = batch.hodge_l1\n",
    "            if hodge_l1.dim() == 1:\n",
    "                hodge_l1 = hodge_l1.reshape(batch_size, -1)\n",
    "            \n",
    "            # Broadcast graph-level features to each node in the graph\n",
    "            # batch_0[i] gives the graph index for node i\n",
    "            hodge_expanded = hodge_l1[batch_0]  # [num_nodes, hodge_k]\n",
    "            \n",
    "            # Concatenate: [num_nodes, dim_hidden + hodge_k]\n",
    "            x_0 = torch.cat([x_0, hodge_expanded], dim=1)\n",
    "        \n",
    "        # Process through network\n",
    "        incidence_hyperedges = getattr(batch, 'incidence_hyperedges', None)\n",
    "        if incidence_hyperedges is None:\n",
    "            incidence_hyperedges = getattr(batch, 'incidence', None)\n",
    "\n",
    "        # Compute hyperedge features\n",
    "        x_1 = None\n",
    "        if incidence_hyperedges is not None:\n",
    "            try:\n",
    "                x_1 = torch.sparse.mm(incidence_hyperedges, x_0)\n",
    "            except Exception:\n",
    "                x_1 = torch.sparse.mm(incidence_hyperedges.T, x_0)\n",
    "        else:\n",
    "            x_1 = torch.zeros(x_0.shape[0], x_0.shape[1], device=x_0.device)\n",
    "\n",
    "        # Apply linear transformations\n",
    "        x_0 = self.linear_0(x_0)\n",
    "        x_0 = torch.relu(x_0)\n",
    "\n",
    "        x_1 = self.linear_1(x_1[:, :self.linear_1.in_features])  # Project if needed\n",
    "        x_1 = torch.relu(x_1)\n",
    "\n",
    "        model_out = {\n",
    "            'labels': batch.y,\n",
    "            'batch_0': batch_0 if 'batch_0' in locals() else getattr(batch, 'batch_0', None),\n",
    "            'x_0': x_0,\n",
    "            'hyperedge': x_1,\n",
    "        }\n",
    "        return model_out\n",
    "\n",
    "print('Improved Backbone with Hodge Feature Fusion defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment: Compare models with and without Hodge feature integration\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for use_hodge in [False, True]:\n",
    "    model_name = f\"Model {'WITH' if use_hodge else 'WITHOUT'} Hodge Features\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Create fresh backbone for this experiment\n",
    "    backbone = MyBackboneWithHodgeFusion(dim_hidden, hodge_k=hodge_k, use_hodge=use_hodge)\n",
    "    readout = PropagateSignalDown(**readout_config)\n",
    "    loss = TBLoss(**loss_config)\n",
    "    feature_encoder = AllCellFeatureEncoder(in_channels=[in_channels], out_channels=dim_hidden)\n",
    "    evaluator = TBEvaluator(**evaluator_config)\n",
    "    optimizer = TBOptimizer(**optimizer_config)\n",
    "    \n",
    "    model = TBModel(\n",
    "        backbone=backbone,\n",
    "        backbone_wrapper=None,\n",
    "        readout=readout,\n",
    "        loss=loss,\n",
    "        feature_encoder=feature_encoder,\n",
    "        evaluator=evaluator,\n",
    "        optimizer=optimizer,\n",
    "        compile=False\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,  # Reduced epochs for faster comparison\n",
    "        accelerator='cpu',\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=5,\n",
    "        enable_model_summary=False,\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, datamodule)\n",
    "    \n",
    "    # Get metrics\n",
    "    train_metrics = trainer.callback_metrics\n",
    "    trainer.test(model, datamodule)\n",
    "    test_metrics = trainer.callback_metrics\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Train Loss': float(train_metrics.get('train_loss_epoch', 0)),\n",
    "        'Test Accuracy': float(test_metrics.get('test/accuracy', 0)),\n",
    "        'Test Loss': float(test_metrics.get('test/loss', 0)),\n",
    "        'Test Precision': float(test_metrics.get('test/precision', 0)),\n",
    "        'Test Recall': float(test_metrics.get('test/recall', 0)),\n",
    "    })\n",
    "\n",
    "# Display comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nKey Observation:\")\n",
    "print(\"If Hodge features are not properly integrated, the model WITH Hodge features\")\n",
    "print(\"may show WORSE test performance due to overfitting on irrelevant feature space.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a207e",
   "metadata": {},
   "source": [
    "## Summary: Hodge L1 Features and Overfitting\n",
    "\n",
    "### Root Causes of Poor Performance:\n",
    "\n",
    "| Issue | Impact | Solution |\n",
    "|-------|--------|----------|\n",
    "| **Features Not Used** | `hodge_emb` computed but ignored by readout & loss | Fuse into node features or custom readout |\n",
    "| **Unintegrated Params** | Extra learnable params → overfitting on noise | Connect to prediction pathway |\n",
    "| **Batching Artifacts** | Graph features concatenated incorrectly | Reshape and broadcast to nodes properly |\n",
    "| **Model Capacity** | More params without improved signal → overfit | Use regularization (dropout, L2) |\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Topological features alone aren't enough** - they must be properly integrated into the model's representation learning process\n",
    "\n",
    "2. **Concatenating features ≠ Learning from them** - simply adding features without ensuring they're used in predictions doesn't help\n",
    "\n",
    "3. **Graph-level features need broadcasting** - Hodge L1 is computed per-graph but predictions happen at node/edge level, so features must be correctly mapped\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "✅ **DO:**\n",
    "- Fuse Hodge features with node embeddings early (as shown in `MyBackboneWithHodgeFusion`)\n",
    "- Validate that features actually affect predictions (check gradients)\n",
    "- Use early stopping and validation curves to detect overfitting\n",
    "- Consider dimensionality reduction of Hodge features if hodge_k is large\n",
    "\n",
    "❌ **DON'T:**\n",
    "- Compute features that aren't used downstream\n",
    "- Add disconnected learnable layers\n",
    "- Ignore the batching structure of PyTorch Geometric\n",
    "- Assume more features = better performance\n",
    "\n",
    "### Testing Hypothesis:\n",
    "\n",
    "Run the comparison experiment above. If results show:\n",
    "- **WITH Hodge**: Better test accuracy → Good integration\n",
    "- **WITH Hodge**: Worse test accuracy → Poor integration (as expected with current code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cff0a4",
   "metadata": {},
   "source": [
    "## Technical Deep Dive: Why Hodge Features Break the Model\n",
    "\n",
    "### The Problem in Your Current Code:\n",
    "\n",
    "```python\n",
    "# Current (BROKEN) approach:\n",
    "hodge_emb = self.hodge_encoder(batch.hodge_l1)  # Computes embeddings\n",
    "model_out['hodge_emb'] = hodge_emb               # Stores them\n",
    "# ❌ But hodge_emb is NEVER used again!\n",
    "```\n",
    "\n",
    "### What Happens During Training:\n",
    "\n",
    "1. **Forward pass**: Hodge encoder learns arbitrary patterns\n",
    "2. **Backward pass**: Gradients from loss DO NOT flow through hodge_encoder\n",
    "3. **Result**: Parameters are essentially frozen (no meaningful updates)\n",
    "4. **Side effect**: Model has to fit everything through other paths, causing overfitting\n",
    "\n",
    "### The Batching Issue:\n",
    "\n",
    "```\n",
    "Individual samples:  hodge_l1[0] has shape [10]  (hodge_k=10)\n",
    "                    hodge_l1[1] has shape [10]\n",
    "                    hodge_l1[2] has shape [10]\n",
    "\n",
    "After batching with from_data_list():\n",
    "batch.hodge_l1 has shape [30]  ← Concatenated!\n",
    "\n",
    "Your code tries:\n",
    "batch_size = 3\n",
    "hodge_l1.reshape(3, -1)  → [3, 10]  ✓ Correct!\n",
    "\n",
    "But association with nodes is wrong:\n",
    "Each graph has different numbers of nodes\n",
    "batch_0[i] tells you which graph node i belongs to\n",
    "You need to use batch_0 to properly broadcast!\n",
    "```\n",
    "\n",
    "### The Correct Integration Pattern:\n",
    "\n",
    "```python\n",
    "# ✅ CORRECT approach - fuse into node features:\n",
    "batch_0 = batch.batch_0  # [num_nodes] - graph index for each node\n",
    "hodge_expanded = batch.hodge_l1[batch_0]  # Broadcast to match nodes\n",
    "\n",
    "# Now gradients flow: loss → x_0 → hodge_expanded → hodge_l1 → params\n",
    "```\n",
    "\n",
    "### Mathematical Perspective:\n",
    "\n",
    "Let $x_{n,i}$ be features of node $n$ in graph $i$, and $h_i$ be Hodge features of graph $i$.\n",
    "\n",
    "**Wrong way** (current):\n",
    "$$\\hat{y}_i = f(x_{i}^{agg})$$ \n",
    "where $h_i$ exists but is never used\n",
    "\n",
    "**Right way** (improved):\n",
    "$$\\hat{y}_i = f([x_{i}^{agg}; h_i])$$\n",
    "where graph-level features are concatenated with aggregated node features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
